---
title: "Statistical Analysis, MSCA 31007, Lecture 5"
author: "Hope Foster-Reyes"
date: "November 26, 2016"
output: pdf_document
geometry: margin=0.75in
---

# Analysis of Residuals of a Linear Model

Understand estimation and inference for simple linear regression, including separating mixed models using volatility clustering.

_Notes_

* _Style Guide:_ https://google.github.io/styleguide/Rguide.xml

* _Packages required: fitdistrplus_

* _Files required: ResidualAnalysisProjectData_2.csv; store in RStudio project directory_

```{r settings, echo=FALSE}
options(scipen = 5)
suppressWarnings(library(fitdistrplus))
```

## Method 1

### 1.1 Project Data

Analyze the second case data from file ResidualAnalysisProjectData_2.csv.

Import and plot the sample data.

```{r import}
# Import linear model ('lm') data
sample.data <- read.csv("ResidualAnalysisProjectData_2.csv")
head(sample.data)
plot(sample.data$Input,sample.data$Output, type="p",pch=19)
par(mfrow=c(1,2))
boxplot(sample.data$Output)
hist(sample.data$Output)
par(mfrow=c(1,1))
sample.n <- length(sample.data$Input)
```

### 1.2 Estimate Linear Model Based on Raw Sample Data

Fit a linear model to the data and plot the sample and the fitted values.

```{r q1.2}
lm.raw <- lm(Output ~ Input, sample.data)
lm.raw$coefficients
matplot(sample.data$Input, cbind(sample.data$Output, lm.raw$fitted.values),
        type = "p", pch = 16, ylab = "Sample and Fitted Values")

lm.raw.beta0 <- lm.raw$coefficients[1]
lm.raw.beta1 <- lm.raw$coefficients[2]
lm.raw.y.fit <- sample.data$Input * lm.raw.beta1 + lm.raw.beta0
lm.raw.resid <- sample.data$Output - lm.raw.y.fit
```

Analyze the results of fitting.

```{r q1.2b}
(lm.raw.summary <- summary(lm.raw))
```

***Interpret the summary of the model.***

#### Call

The `Call` section simply reminds us of our call to the function, which provides the data and specifies which variable is our independent (predictor or input variable) and which is our dependent (response or output variable). Note that we denote this by first listing our dependent variable, followed by a tilde symbol, and finally our independent variable.

#### Residuals

This is followed by the `Residuals` section which provides the five number summary of the residuals. What may be interesting to us here is that the summary is nearly symmetrical, the quartiles seem to divide the data nearly evenly, and the two quartiles close to the median have a smaller spread than those further away, indicating that the data is not spread uniformly throughout the distribution but is clustered near the center as in a normal distribution. Some quick plots further explore the residuals:

```{r q1.2-plot}
par(mfrow=c(1,2))
boxplot(lm.raw.resid)
hist(lm.raw.resid)
par(mfrow=c(1,1))
```

#### Coefficients

Next, the `Coefficients` section lists our estimated linear coefficients. The first will always be our Intercept (which, essentially, is a coefficient to the number 1), followed by the coefficients of our remaining predictor variables. In this case there is only one predictor variable, which we labeled Input. Our Input coefficient, is positive, leading to a positive correlation between Input (X) and Output (Y), which is also demonstrated by our plots above.

The `Pr(>|t|)` column of the Coefficients section tells us the probability of obtaining our data if the null hypothesis were true, in this case if the coefficients were actually zero. In our case the p-value of $a$ or our Input coefficient is quite low and statistically significant, indicating the utility of our model. 

We can be less confident of our intercept, with a p-value of 0.523. This may warrant further investigation. 

#### Model

Our summary output includes this calculation of the residual standard deviation, or $\sigma$. It is worth noting that [R documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html) states that the term "residual standard error" that we see in our `summary` output is a misnomer, and the proper term is residual standard deviation.

This figure, `r round(lm.raw.summary$sigma, 3)`, describes the amount that the residuals vary or spread from their mean, which is theoretically zero, and hence the amount that our output variable varies or spreads from a pure linear relationship with our input variable(s). Placing this value in the context of our model, we have the following equation describing our estimate of the variation in Output:

$Output = `r round(lm.raw.beta1, 3)` \cdot Input + `r round(lm.raw.beta0, 3)` + e$

$e \sim Norm(0, `r round(lm.raw.summary$sigma, 3)`)$

R-squared in the summary output (also notated as $r^{2}$ or $\rho^{2}$) is our "coefficient of determination" or "squared correlation coefficient". Whereas the correlation coefficient, "r" measures the strength and the direction of a linear relationship between two variables, the square of the correlation coefficient, "r squared", which we see in our summary, measures the proportion of our data that is explained by the linear relationship. Thus r-squared can be considered one estimate of the "predictive power" of our model. 

In this case roughly 84% of the variation in our Output is explained by our linear model of:

The p-value of the model itself is found in the F-statistic section. Our p-value is quite low indicating statistical significance in our linear model.

Thus our model explains roughtly 84% of our variation and has a statistically significant relationship between Input and Output, however our intercept estimate is not statistically significant. In other words, we are quite confident of the slope of our line, yet we're not so sure about its intercept. 

The wide gap in p-value between slope and intercept is worth further investigation.

***Analyze the residuals, plot them, and their probability density function.***

```{r q1.2-resid}
all.equal(as.vector(lm.raw$residuals), lm.raw.resid)
all.equal(as.vector(lm.raw$fitted.values), lm.raw.y.fit)

lm.raw.resid <- lm.raw$residuals
plot(sample.data$Input, lm.raw.resid)

lm.raw.resid.density <- density(lm.raw.resid)
plot(lm.raw.resid.density, ylim = c(0, 0.5))
lines(lm.raw.resid.density$x,
      dnorm(lm.raw.resid.density$x, mean = mean(lm.raw.resid), sd = sd(lm.raw.resid)))
```

***What does the pattern of residuals and the pattern of the data tell you about the sample?***

The plot of the data show a distinct split of the y values into two parts as the x values become higher. This same pattern can be seen in the residuals though less distinctly. When looking at the density chart of the residuals, we see multiple peaks indicating we are possibly looking at more than one distribution.

We can guess that we are looking at a mixed model of two distict distributions. 

***What kind of mixture of two models do you see in the data?***

Judging from the plot of the data, these separate populations (models) have a different slope, unlike our previous model in which they appeared to have the same slope but different intercepts. Both appear to be normal (Gaussian).

### 1.3 Create training samples for separation of mixed models

Try to separate the subsamples with different models.

Create training sample with `Input >= 5` and separate the points above the fitted line and below.

```{r q1.3a}
# Create vectors for 3 training samples, each with a column of our Input values,
#   followed by a column of NA values
sample.na <- rep(NA, sample.n)
sample.training <- data.frame(trainInput = sample.data$Input, trainOutput = sample.na)
sample.training.steep <- data.frame(trainSteepInput = sample.data$Input, 
                                    trainSteepOutput = sample.na)
sample.training.flat <- data.frame(trainFlatInput = sample.data$Input, 
                                   trainFlatOutput = sample.na)

head(cbind(sample.data, sample.training, sample.training.steep, sample.training.flat))
```

Select parts of the sample with `Input` greater than 5 and `Output` either above the estimated regression line or below it.

```{r q1.3b}
# Create a selector for Input >= 5, focusing only on the right area of the plot
selector.training.base <- sample.data$Input >= 5

# Create a selector meeting the above criteria, plus Output above the fitted y line
selector.training.steep <- selector.training.base & (sample.data$Output > lm.raw$fitted.values)

# Create a selector meeting the above criteria, plus Output above the fitted y line
selector.training.flat <- selector.training.base & (sample.data$Output <= lm.raw$fitted.values)

# Replace the NA values in our earlier training data frames with the qualified Output values
#   based on our selectors
sample.training[selector.training.base, 2] <- sample.data[selector.training.base, 2]
sample.training.steep[selector.training.steep, 2] <- sample.data[selector.training.steep, 2]
sample.training.flat[selector.training.flat, 2] <- sample.data[selector.training.flat, 2]

head(sample.training)
head(cbind(sample.data, sample.training, sample.training.steep, sample.training.flat), 10)

plot(sample.training$trainInput, sample.training$trainOutput, 
     pch = 16, ylab = "Training Sample Output", xlab="Training Sample Input")
points(sample.training.steep$trainSteepInput, sample.training.steep$trainSteepOutput, pch = 20, col = "green")
points(sample.training.flat$trainFlatInput, sample.training.flat$trainFlatOutput, pch = 20, col = "blue")
```

### 1.4 Fit linear models to the training samples

Fit linear models to both training samples.

```{r q1.4}
lm.training.steep <- lm(trainSteepOutput ~ trainSteepInput, sample.training.steep)
lm.training.steep.summary <- summary(lm.training.steep)
lm.training.flat <- lm(trainFlatOutput ~ trainFlatInput, sample.training.flat)
lm.training.flat.summary <- summary(lm.training.flat)

lm.training.steep.summary
lm.training.flat.summary

lm.pf <- pf(lm.raw.summary$fstatistic[1], 
            lm.raw.summary$fstatistic[2], 
            lm.raw.summary$fstatistic[3],lower.tail = FALSE)
lm.training.steep.pf <- pf(lm.training.steep.summary$fstatistic[1], 
                        lm.training.steep.summary$fstatistic[2], 
                        lm.training.steep.summary$fstatistic[3],lower.tail = FALSE)
lm.training.flat.pf <- pf(lm.training.flat.summary$fstatistic[1], 
                       lm.training.flat.summary$fstatistic[2], 
                       lm.training.flat.summary$fstatistic[3],lower.tail = FALSE)

t00.2 <- round(lm.raw$coefficients[1], 3)
t00.3 <- round(lm.training.steep$coefficients[1], 3)
t00.4 <- round(lm.training.flat$coefficients[1], 3)
t01.2 <- round(lm.raw$coefficients[2], 3)
t01.3 <- round(lm.training.steep$coefficients[2], 3)
t01.4 <- round(lm.training.flat$coefficients[2], 3)
t1.2 <- round(lm.raw.summary$sigma, 3)
t1.3 <- round(lm.training.steep.summary$sigma, 3)
t1.4 <- round(lm.training.flat.summary$sigma, 3)
t2.2 <- round(lm.raw.summary$r.squared, 3)
t2.3 <- round(lm.training.steep.summary$r.squared, 3)
t2.4 <- round(lm.training.flat.summary$r.squared, 3)
t2b.2 <- round(lm.raw.summary$adj.r.squared, 3)
t2b.3 <- round(lm.training.steep.summary$adj.r.squared, 3)
t2b.4 <- round(lm.training.flat.summary$adj.r.squared, 3)
t3.2 <- round(lm.raw.summary$coefficients[1,4], 3)
t3.3 <- round(lm.training.steep.summary$coefficients[1,4], 3)
t3.4 <- round(lm.training.flat.summary$coefficients[1,4], 3)
t4.2 <- round(lm.raw.summary$coefficients[2,4], 3)
t4.3 <- round(lm.training.steep.summary$coefficients[2,4], 3)
t4.4 <- round(lm.training.flat.summary$coefficients[2,4], 3)
t5.2 <- round(lm.raw.summary$fstatistic[1])
t5.3 <- round(lm.training.steep.summary$fstatistic[1])
t5.4 <- round(lm.training.flat.summary$fstatistic[1])
```

Estimate | Linear Model | Training Sample Steep | Training Sample Flat
---------|--------------|-----------------------|----------------------
`r dimnames(lm.raw.summary$coefficients)[[1]][1]` estimate | `r t00.2` | `r t00.3` | `r t00.4`
`r dimnames(lm.raw.summary$coefficients)[[1]][2]` estimate | `r t01.2` | `r t01.3` | `r t01.4`
$\sigma_{\varepsilon}$ | `r t1.2` | `r t1.3` | `r t1.4`
$\rho^{2}$ | `r t2.2` | `r t2.3` | `r t2.4`
$\rho^{2} adjusted$ | `r t2b.2` | `r t2b.3` | `r t2b.4`
`r dimnames(lm.raw.summary$coefficients)[[1]][1]` p-value | `r t3.2` | `r t3.3` | `r t3.4`
`r dimnames(lm.raw.summary$coefficients)[[1]][2]` p-value | `r t4.2` | `r t4.3` | `r t4.4`
F-statistic | `r t5.2` | `r t5.3` | `r t5.4`
F p-value | `r round(lm.pf, 4)` | `r round(lm.training.steep.pf, 4)` | `r round(lm.training.flat.pf, 4)`

***Interpret the summaries of both models.***

??? XXX ???

### Separate the original data using the training samples

Plot the entire sample with the fitted regression lines estimated from both training subsamples.

```{r training-plot}
lm.training.steep.predict <- predict(lm.training.steep, 
                             data.frame(trainSteepInput=sample.data$Input), 
                             interval="prediction")
lm.training.flat.predict <- predict(lm.training.flat, 
                            data.frame(trainFlatInput=sample.data$Input), 
                            interval="prediction")
plot(sample.data$Input, sample.data$Output, type="p", pch=19)
lines(sample.data$Input, lm.training.steep.predict[,1], col="red", lwd=3)
lines(sample.data$Input, lm.training.flat.predict[,1], col="green", lwd=3)
```

Separate the entire sample using the estimated train linear models.

```{r training-separate}
# Create regression lines for the steeper training sample and the flatter training sample
line.steep <- sample.data$Input * lm.training.steep$coefficients[2] + lm.training.steep$coefficients[1]
line.flat <- sample.data$Input * lm.training.flat$coefficients[2] + lm.training.flat$coefficients[1]

# Define the distances from each Output point to both estimated training lines
deviation.steep <- abs(sample.data$Output - line.steep)
deviation.flat <- abs(sample.data$Output - line.flat)

# Define separating sequence which equals TRUE if observation belongs to model with steeper slope 
#   and FALSE otherwise.
selector.steep <- deviation.steep < deviation.flat

# Create vectors for 2 subsamples, each with a column of our Input values,
#   followed by a column of NA values
sample.steep <- data.frame(steepInput = sample.data$Input, 
                           steepOutput = sample.na)
sample.flat <- data.frame(flatInput = sample.data$Input, 
                          flatOutput = sample.na)

# Replace the NA values in our earlier training data frames with the qualified Output values
#   based on our selector sequence
#  (Note: In this case we are creating the entire sample, not just the portion >5 as above.)
sample.steep[selector.steep, 2] <- sample.data[selector.steep, 2]
sample.flat[!selector.steep, 2] <- sample.data[!selector.steep, 2]

head(cbind(sample.data, sample.steep, sample.flat))
```

Plot the two samples.

```{r separated-plot}
matplot(sample.data$Input,
        cbind(sample.data$Output, sample.steep$steepOutput, sample.flat$flatOutput),
        type="p", col=c("black","green","blue"), pch=16, ylab="Separated Subsamples")
```

Find the mixing probability. Run binomial test for the null hypothesis p=0.5 and two-sided alternative “p is not equal to 0.5”. 

```{r separated-probability}
(mixing.probability.steep <- sum(selector.steep) / length(selector.steep))

binom.test(sum(selector.steep), sample.n, p = 0.5)
```

***Interpret the output of binom.test.***

??? XXX

***What do you conclude from the test results?***

??? XXX

1.5 Fit models to separated samples

Estimate linear models for separated subsamples.

```{r q1.5}
lm.steep <- lm(sample.steep$steepOutput ~ sample.steep$steepInput, sample.steep)
lm.steep.summary <- summary(lm.steep)
lm.flat <- lm(sample.flat$flatOutput ~ sample.flat$flatInput, sample.flat)
lm.flat.summary <- summary(lm.flat)
```

Print out coefficients for both separated models. Check the summaries.

```{r q1.5-coef}
rbind("Steep Coefficients"=lm.steep$coefficients, "Flat Coefficients"=lm.flat$coefficients)
lm.steep.summary$r.squared
lm.flat.summary$r.squared
```

### 1.6 Analyze the residuals

Compare the residuals of separated models with the residuals of the single model.

```{r q1.6-plot}
lm.unscrambled.resid <- c(lm.steep.summary$residuals, lm.flat.summary$residuals)
matplot(sample.data$Input,
        cbind(lm.unscrambled.resid, lm.raw.resid),
        type="p", pch=c(19,16), ylab="Residuals before and after unscrambling")
legend("bottomleft", legend=c("Before","After"), col=c("red","black"),pch=16)
```

Estimate standard deviations of the residuals.

```{r q1.6-sd}
apply(cbind("Residuals After"=lm.unscrambled.resid, 
            "Residuals Before"=lm.raw.resid),
      2, sd)
```

Check assumptions about residuals.

```{r q1.6-test}
hist(lm.unscrambled.resid)

(lm.unscrambled.resid.param <- fitdistr(lm.unscrambled.resid, "normal"))
ks.test(lm.unscrambled.resid, "pnorm", lm.unscrambled.resid.param$estimate[1])
qqnorm(lm.unscrambled.resid)
qqline(lm.unscrambled.resid)
```

Finally, print out the slopes and intercepts of both models.

```{r q1.6-coef}
t00.2 <- round(lm.raw$coefficients[1], 3)
t00.3 <- round(lm.steep$coefficients[1], 3)
t00.4 <- round(lm.flat$coefficients[1], 3)
t01.2 <- round(lm.raw$coefficients[2], 3)
t01.3 <- round(lm.steep$coefficients[2], 3)
t01.4 <- round(lm.flat$coefficients[2], 3)
```

Estimate | Linear Model | Linear Model Steep | Linear Model Flat
---------|--------------|-----------------------|----------------------
`r dimnames(lm.raw.summary$coefficients)[[1]][1]` estimate | `r t00.2` | `r t00.3` | `r t00.4`
`r dimnames(lm.raw.summary$coefficients)[[1]][2]` estimate | `r t01.2` | `r t01.3` | `r t01.4`

## 2 Alternative Method Based on Volatility Clustering

### 2.1 Explore the relationship between slope and squared deviation.

If the sample is $<y1,…,yn>$ then estimate of variance is built by averaging terms $(y_{i}-\bar{y})^{2}$ as

$$\hat{\sigma}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}$$

Make a plot of squared deviations $z_{i}=(y_{i}-\bar{y})^{2}$

```{r q2-plot}
lm.raw.dev.sq <- (sample.data$Output - mean(sample.data$Output))^2
plot(sample.data$Input, lm.raw.dev.sq,
     type="p", pch=19, ylab="Squared Deviations")
```

Data points on this plot seem to cluster in two or more parabolic shapes.

An alternative approach to unmixing the models can be based on separating two parabolas on the data plot. 

Let's demonstrate the impact of changing the slope in our linear model on our squared deviations.

```{r q2-plot-sq-dev, fig.height=8}
demo.x <- runif(1000, -.5, .5)
demo.resid.sd <- 0.5
demo.resid <- rnorm(1000, sd = demo.resid.sd)
demo.beta0 <- 0

DemoSquaredDev <- function(beta1) {
  demo.y <- (beta1 * demo.x) + demo.beta0 + demo.resid
  demo.y.mean <- mean(demo.y)
  demo.y.dev.sq <- (demo.y - demo.y.mean)^2
  plot(demo.x, demo.y, ylim = c(-5,5),
       main = paste("X, Y Plot\nSlope =", beta1))
  abline(h = demo.y.mean)
  plot(demo.x, demo.y.dev.sq, 
       main = paste("Squared Deviation\nVariance ", round(var(demo.y),2), sep=""))
  abline(h = var(demo.y))
}

# Plot squared deviations in simple linear models with varying slopes
par(mfrow=c(4,2))
DemoSquaredDev(1)
DemoSquaredDev(2)
DemoSquaredDev(4)
DemoSquaredDev(8)
par(mfrow=c(1,1))
```

***Explain how increased slope affects variance of the output and the pattern of the variable $z_{i}$ (squared y deviation).***

Our simple linear model is built on the equation below, where $\varepsilon$ (Epsilon) is a normal variable representing randomness in our model:

$Y = \beta_{0} + \beta_{1} X + \varepsilon$

The demonstration above shows, in the right-hand column, the plot of squared deviations of a simple linear model (represented by the left-hand column) with changing slope, or $\beta_{1}$ coefficient. The horizontal lines show, respectively, the mean of Y on the left and the variance of Y on the right.

* Variance

An increased slope, $\beta_{1}$ in the above equation will clearly increase the variance of Y, our response or ouput variable, as it is a multiplier for Y.

* Deviation

If we consider that the mean of Y is a point "halfway between" our Y values, then we can appreciate that in a linear model the length of the deviations from this mean will be larger for our lowest X values, becoming smaller as we approach our central X vales, becoming smallest at the point where our mean Y crosses our mean Y, and then becoming larger as our X values increase.

If we remove our Epsilon randomness this is even more evident:

```{r q2-plot-abs-dev}
demo.beta1 <- 4

demo.y <- (demo.beta1 * demo.x) + demo.beta0 
demo.y.mean <- mean(demo.y)
demo.x.mean <- mean(demo.x)

plot(demo.x, demo.y, 
     main = paste("X, Y Plot\nSlope =", demo.beta1))
abline(h = demo.y.mean, col="red")
abline(v = demo.x.mean, col="blue")
text(min(demo.x) + 0.1, demo.y.mean + 0.1, "y mean", col="red")
text(demo.x.mean + 0.1, max(demo.y) - 0.05, "x mean", col="blue")

# Compare absolute deviation with squared deviation
par(mfrow=c(1,2))

demo.y.dev.abs <- abs(demo.y - demo.y.mean)
demo.y.md <- mean(demo.y.dev.abs)
plot(demo.x, demo.y.dev.abs, xlim = c(-0.5, 0.5),
     main = paste("Absolute Deviation\nMean Abs Dev ", round(demo.y.md,2), sep=""))
abline(h = var(demo.y))

demo.y.dev.sq <- (demo.y - demo.y.mean)^2
plot(demo.x, demo.y.dev.sq, xlim = c(-0.5, 0.5),
       main = paste("Squared Deviation\nVariance ", round(var(demo.y),2), sep=""))
abline(h = var(demo.y))

par(mfrow=c(1,1))
```

Above we see that the absolute deviation for our sloped y line from its mean is linear, following the pattern described above, whereas the squared deviation forms a parabola (due to the properties of a squared variable).

Further, our matrix of varying slopes earlier shows that as the slope of the linear model becomes larger, the shape of the parabola representing the squared deviation of Y, our response variable, becomes steeper and more concave.

***What are the differences between the shapes of parabolas corresponding to a steeper slope versus flatter slope?***

The parabolas with a steeper slope correspond to a larger slope in the linear model and a larger variation in y.

### 2.2 Separate the models using this approach.

So, we have encountered a new method for splitting our model. 

* In an earlier assignment, we fitted a y line for our model and divided the y values according to whether they were above or below this fitted line.

* For this method we will calculate a parabola representing the squared deviation of our entire sample, and then divide our y values according to whether their squared deviation falls inside "steeper than" or outside "flatter than" our fit parabolic line.

***Find the parabola corresponding to fitted model `m1`.***

We can use the following to estimate the parabola that we will use to divide our model:

$Y = \beta_{0} + \beta_{1} X + \varepsilon$

$y_{i} = \beta_{0} + \beta_{1} x_{i} + \varepsilon_{i}$

$\bar{y} = \beta_{0} + \frac{\beta_{1}}{n} \sum{x_{i}} + \frac{1}{n} \sum{\varepsilon_{i}}$

Or we can simply estimate the parabola using the fitted y values provided earlier by `lm()`. The below shows these approaches and their equivalence.

```{r q2.2-parabola}
lm.raw.y.fit.mean.v1 <- mean(lm.raw.y.fit)
lm.raw.y.fit.mean.v2 <- lm.raw.y.fit.mean.v1 + mean(lm.raw.resid)
lm.raw.y.fit.mean.v3 <- lm.raw.beta0 + lm.raw.beta1 * mean(sample.data$Input) + mean(lm.raw.resid)

c(lm.raw.y.fit.mean.v1, lm.raw.y.fit.mean.v2, lm.raw.y.fit.mean.v3)
(all.equal(var(c(lm.raw.y.fit.mean.v1, lm.raw.y.fit.mean.v2, lm.raw.y.fit.mean.v3)), 0))

lm.raw.dev.sq.fit <- (lm.raw.y.fit - mean(lm.raw.y.fit))^2

plot(sample.data$Input, lm.raw.dev.sq,
     type="p", pch=19, ylab="Squared Deviations")
points(sample.data$Input, lm.raw.dev.sq.fit, pch=19, col="red")
```

Now let's create a new selector sequence that sets a value of `TRUE` for y values we suspect belong to a subset with a steeper slope and `FALSE` for values we suspect belong to a subset with a flatter slope -- this time basing our selector on their relationship to our fit squared deviation parabola ("Method 2") rather than our fit y value line ("Method 1").

```{r q2.2-selector}
selector.steep.var <- lm.raw.dev.sq > lm.raw.dev.sq.fit
head(selector.steep.var, 10)
```

Separate the sample into steeper and flatter part and plot.

```{r q2.2-separate}
# Create vectors for 2 subsamples, each with a column of our Input values,
#   followed by a column of NA values
sample.steep.var <- data.frame(steepInput.var = sample.data$Input, 
                               steepOutput.var = sample.na)
sample.flat.var <- data.frame(flatInput.var = sample.data$Input, 
                              flatOutput.var = sample.na)

# Replace the NA values in our earlier data frames with the qualified Output values
#   based on our selector sequence
sample.steep.var[selector.steep.var, 2] <- sample.data[selector.steep.var, 2]
sample.flat.var[!selector.steep.var, 2] <- sample.data[!selector.steep.var, 2]

head(cbind(sample.data, sample.steep.var, sample.flat.var),10)

# Plot clusters of the variance data and the separating parabola
plot(sample.data$Input, lm.raw.dev.sq,
     type="p", pch=19, ylab="Squared Deviations")
points(sample.data$Input, lm.raw.dev.sq.fit, pch=19, col="red")
points(sample.data$Input[selector.steep.var],
       lm.raw.dev.sq[selector.steep.var], pch=19, col="blue")
points(sample.data$Input[!selector.steep.var],
       lm.raw.dev.sq[!selector.steep.var], pch=19, col="green")
```

Plot the unscrambled subsamples, include the original entire sample as a check.

```{r q2.2-separated-plot}
matplot(sample.data$Input, 
        cbind(sample.data$Output,
              sample.steep.var$steepOutput.var, 
              sample.flat.var$flatOutput.var),
        type="p", col=c("black","green","blue"), pch=16,
        ylab="Separated Subsamples")
```

Note that observations corresponding to the minimum of the variance data are difficult to separate. 
Consider omitting some observations around that point. For example, make omitted interval equal to LeftBound=-0.5, RightBound=0.5.

```{r}
# Create a selector to exclude the middle
selector.exclude.lbound <- mean(sample.data$Input) - 0.5
selector.exclude.rbound <- mean(sample.data$Input) + 0.5
selector.exclude.mid <- (sample.data$Input <= selector.exclude.lbound)|
                        (sample.data$Input >= selector.exclude.rbound)

matplot(sample.data$Input[selector.exclude.mid], 
        cbind(sample.data$Output[selector.exclude.mid],
              sample.steep.var$steepOutput.var[selector.exclude.mid], 
              sample.flat.var$flatOutput.var[selector.exclude.mid]),
        type="p", col=c("black","green","blue"), pch=16,
        ylab="Separated Subsamples")
```

### 2.3 Fit linear models to the separated samples. 

Plot the data and the estimated regression lines.

```{r q2.3}
lm.steep.var <- lm(sample.steep.var$steepOutput.var ~ sample.steep.var$steepInput.var, sample.steep.var)
lm.steep.var.summary <- summary(lm.steep.var)
lm.flat.var <- lm(sample.flat.var$flatOutput.var ~ sample.flat.var$flatInput.var, sample.flat.var)
lm.flat.var.summary <- summary(lm.flat.var)

lm.steep.var.predict <- predict(lm.steep.var, 
                                data.frame(steepInput.var=sample.data$Input),
                                interval = "prediction")
lm.flat.var.predict <- predict(lm.flat.var, 
                               data.frame(flatInput.var=sample.data$Input),
                               interval = "prediction")

plot(sample.data$Input, sample.data$Output, type="p")
lines(sample.data$Input, lm.steep.var.predict[,1], col="red", lwd=3)
lines(sample.data$Input, lm.flat.var.predict[,1], col="green", lwd=3)
```

Print estimated parameters and summaries of both models.

```{r q2.3-coef}
rbind("Steep Coefficients"=lm.steep.var$coefficients, "Flat Coefficients"=lm.flat.var$coefficients)
lm.steep.var.summary
```



